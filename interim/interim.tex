\documentclass[11pt]{article}

% First load extension packages
\usepackage[a4paper,margin=25mm]{geometry}    % page layout
\usepackage{setspace} \onehalfspacing         % line spacing
\usepackage{amsfonts,amssymb,amsmath}         % useful math extensions
\usepackage{graphicx}                         % graphics import
\usepackage{siunitx}                          % easy SI units
\usepackage{url}
\usepackage{wrapfig}
\usepackage{float}
\usepackage[small,compact]{titlesec}
\usepackage{paralist} %and then replace \begin{itemize}\end{itemize} with \begin{compactitem}\end{compactitem}
\usepackage{natbib} \setlength{\bibsep}{0.0pt}
\usepackage{booktabs}
\usepackage[table,xcdraw]{xcolor}
\usepackage{tabularx}
\usepackage{graphicx}
\usepackage{adjustbox}

% Change paragraph indentation
\setlength{\parskip}{10pt}
\setlength{\parindent}{0pt}

% User-defined commands
\newcommand{\diff}[2]{\frac{\mathrm{d}{#1}}{\mathrm{d}{#2}}}
\newcommand{\ddiff}[2]{\frac{\mathrm{d}^2{#1}}{\mathrm{d}{#2}^2}}
\newcommand{\pdiff}[2]{\frac{\partial{#1}}{\partial{#2}}}
\newcommand{\pddiff}[2]{\frac{\partial^2{#1}}{\partial{#2}^2}}
\newcommand{\pdiffdiff}[3]{\frac{\partial^2{#1}}{\partial{#2}\partial{#3}}}
\renewcommand{\vec}[1]{\boldsymbol{#1}}
\newcommand{\Idx}{\;\mathrm{d}x}
\newcommand{\Real}{\mathbb{R}}
\newcommand{\Complex}{\mathbb{C}}
\newcommand{\Rational}{\mathbb{Q}}
\newcommand{\Integer}{\mathbb{Z}}
\newcommand{\Natural}{\mathbb{N}}

% topmatter
\title{3D Object Detection in the Wild}

\author{Matt Clifford \\ Supervised by Dr R.\ Santos-Rodriguez}

\date{\today}

% main body
\begin{document}
\maketitle

\section{Introduction}
Object detection is important for computers to understand the real world. For example, helping a self driving car know what objects are in it's planned path. 2D object detection is where objects are classified and located in 2D space, an RGB image. However, since the world lies in 3 dimensions, this 2D simplification of the world isn't enough; if there is a cyclist on the road, the self driving car needs to know accurately where they are, to avoid a collision when overtaking, therefore there is a high demand for accurate 3D detectors. A lot of focus with 3D object detection has been with respect to self driving cars. Although they are in a `wild' and unpredictable environment, they are confined to roads. This makes 3D object detection systems for them very specific, as they need to follow strict rules, detect specific objects and are trained on days of driving footage captured from expensive and detailed LIDAR sensors. 

With recent advancements in mixed reality technology from Google glass and Microsoft HoloLens, there is an interest in the use of mixed reality to help create `smart spaces'. Where mixed reality users are informed and guided through environments unknown to them. This addresses circulation issues, or aids the visually and/or navigationally impaired.

Fracture Reality \cite{fracture}, have expressed specific interest in 3D object detection. They specialise in creating bespoke mixed reality software for both private and government sectors. They mostly work with mixed reality visualisations of maps of an environment, for example to aid the control centres in airports \cite{youtube}. Although they have many projects that would benefit from 3D object detection, an ongoing project which is investigating how effective the use of mixed reality is in tackling navigational issues, where fire fighters are aided by a mixed reality headset when entering and exiting new buildings. The use of 3D object detection would identify and locate objects such as stairs, doors and elevators. This would aid the fire fighter in identifying if these objects are correct pathway for them, without them having detailed prior knowledge of the building, since a map can be stored within the mixed reality device.

Fracture Reality are able to create some object specific use case data, but in the region of hundreds of examples due to the expensive and time consuming nature of hand labelling 3D data. This is the main bottleneck in creating effective object detection systems. As well, due to the specificity of each task, finding existing object datasets in 3D is unlikely. This makes training a detector on 3D data alone for every new object infeasible, especially considering the size of the datasets required in deep learning. 2D data however is more widely abundant and relatively cheap and quicker to obtain, which has potential to aid 3D tasks. The need for a system that can detect objects given as little training examples as possible which also makes use of 2D data is therefore needed. Also, since the only certainty is that the objects are going to be in a real life setting, the 3D object detection must be robust and able to cope in the wild, where object labels might not be clear and can change over time. This can be achieved by utilising general knowledge learnt from similar or relevant tasks and applying it to a new specific task of interest, known as transfer learning \cite{DeCAF}. This is where deep neural networks are pre-trained on general tasks where data is sufficiently available to train a robust and general system are then re-purposed and re-trained using data specific for the new task. This works due to earlier layers in networks extracting more general features from the data, which are applicable to many similar tasks.


\section{Literature review}
\subsection*{2D Image Classification}
In 2012, Alex Krizhevsky et al. revolutionised computer vision with a convolution neural network (CNN), inspired by \cite{Yann}. It performed image classification on the ImageNet dataset \cite{ILSVRC15}. The CNN, named AlexNet, consists of 5 convolution layers followed by 3 fully-connected layers. It won ImageNet's ILSVRC-2010 and ILSVRC-2012 image classification contests \cite{alex_net}. In \cite{alex_net}, they claim `All of our experiments suggest that our results can be improved simply by waiting for faster GPUs and bigger datasets to become available', and since AlexNet's success there have been consistent advancements in CNN state of the art. In 2014, \cite{VGG16} propose a CNN named VGG16, consisting of 16 convolution layers followed by 3 fully-connected layers. VGG16 achieves a top-5 error rate of 7.4\% on ILSVRC-2014 compared to AlexNet's 17.0\%. \cite{VGG16} achieves this performance boost by using smaller convolution filters, 3x3 compared to AlexNet's 11x11 which decreases the number of weights to train at each convolution layer, alongside a deeper convolution architecture, which can extract deeper relevant semantic meaning from the images. Further performance improvements were made by the use of `inception models' \cite{inception}\cite{inceptionV2}, which stack the outputs of several convolutions of the same input, followed by filter concatenation. As well as `residual learning' \cite{ResNet}, which connects the outputs of multiple convolution layer. \cite{Incep_ResNet} formulates a combination of `inception' and `residual' models. Further improvements to 2D image classification include \cite{neural_search} \cite{scaleable_image}.

\subsection*{2D Object Detection}
To identify the location of objects in images, \cite{RNN} uses a regional CNN (RCNN) model. Regional proposals of the image, which are run though a modified AlexNet. The positive classification results from the regional proposals are then adjusted using a linear regression model to obtain better object bounding boxes. Computational speed ups are proposed in Fast-RCNN \cite{fast_RNN}, which pools the regional proposals. Further computational speed ups are proposed in Faster-RCNN \cite{faster_RNN}, which combines the selective search regional proposals into the CNN. \cite{YOLO} proposes a grid search grid method rather than the more expensive regional proposals approach, known as `you only look once' (YOLO). YOLO is faster than Faster-RCNN, making it more suitable for real time application, but it comes at the cost of slightly worst performance.

\subsection*{3D Object Detection}
\cite{complex_YOLO} proposes a method of using the YOLO object detection approach in 3D space named Complex-YOLO, using only point cloud data from LIDAR depth sensors. Complex-YOLO uses a Euler-Region Proposal Network which estimates the orientation of objects by adding an imaginary and real part for each proposal box box. This results in 5 times speed up in object detection from the previous state of the art, with on par or better accuracy evaluated on data from KITTI benchmark suit \cite{KITTI}. Other state of the art methods based the KITTI benchmark suite include \cite{point_fusion}\cite{multi_fusion}\cite{fast_furious}\cite{VoxelNet}. 

Some methods focus on using detailed, accurate point cloud objects as input \cite{subgroup_voting}\cite{pose_RGBD}\cite{real_time_single} from datasets such as \cite{3D_dataset}. Using this type of model directly is unsuitable for mixed reality, as there is little background variation which makes them not robust enough for detection in the wild. 

\cite{PointNets} and \cite{frustum} use 2D object detectors to aid the regional proposal of the 3D object, by searching only the 3D space in the point cloud occupied from the projected frustum obtained from the 2D object detector. This vastly reduces the search space. Resulting in reduced misclassification, due to the higher accuracy of 2D detection, especially if the 3D object suffers from occlusions or has a sparse representation. Speed is also improved when compared to using point cloud data alone, due to the pre determination of the object class and reduced search space for the 3D detector. An alternative approach could combine 2D object mask detectors\cite{first_person_mask}\cite{mask_RCNN} with the 3D projection of the mask to help further refine the 3D object search space. \cite{latent_surface} uses latent support surfaces for 3D object detection on the SUNRGBD dataset. Another notable 3D object detector that use the SUNRGBD dataset is \cite{SnapNet}. 

\subsection*{Transfer Learning}
Transfer learning is a well studied area of deep learning \cite{DeCAF}\cite{survery_on_transfer}\cite{how_transferable}, where a network trained for a specific task is re-purposed for a similar task. This is often achieved by truncating the last few layers of a pre-trained network where the network is specific to the trained task, and keeping the starting layers that have more general representations. Since the start of the network is already trained for general tasks relevant to both the original training task and the new desired task, only the last few layers need to be re-trained for the new task. This can be done using considerably less training data than the original network was trained with. Transfer learning could be used to help solve the problem of using as little 3D data as possible to train a 3D object detector. However, when using as little data as possible during the retraining part of transfer learning, the original network needs to be trained for a task as similar as possible as the re-purposed task.

\subsection*{Synthetic Data}
\cite{cut_paste} proposes a `cut and paste' style approach to synthesising 2D object detection datasets. First an object mask is predicted for the object, which is then applied to the image to `cut and paste' the object into background scenes. Occlusions, truncations and blends are then applied to the object, helping it fit more naturally into the scene. This address' the problem of not being able to annotate or collect enough data by hand. \cite{synthetic_train} shows that training scene detectors on synthetic data produces comparable results on real life tests, with object detectors trained on the SUNRGBD state of the art dataset.


\section{Project plan}
\begin{table}[H]
\resizebox{\textwidth}{!}{%
\begin{tabular}{
>{\columncolor[HTML]{EFEFEF}}l ll}
{\color[HTML]{333333} Week} & \cellcolor[HTML]{EFEFEF}{\color[HTML]{333333} Task} & \cellcolor[HTML]{EFEFEF}{\color[HTML]{333333} Relevance to project} \\ \hline
\multicolumn{1}{|l|}{\cellcolor[HTML]{EFEFEF}3} & \multicolumn{1}{l|}{Formalise project problem} & \multicolumn{1}{l|}{Important to have solid project foundations} \\ \hline
\multicolumn{1}{|l|}{\cellcolor[HTML]{EFEFEF}4/5/6} & \multicolumn{1}{l|}{Review literature} & \multicolumn{1}{l|}{\begin{tabular}[c]{@{}l@{}}Will use anything relevant to help with the problem \\ and get a feel for what is realistic\end{tabular}} \\ \hline
\multicolumn{1}{|l|}{\cellcolor[HTML]{EFEFEF}7} & \multicolumn{1}{l|}{Collect/ process dataset(s)} & \multicolumn{1}{l|}{\begin{tabular}[c]{@{}l@{}}Essential for training any system. Making a good \\ pipeline for data enables fast and affective research\end{tabular}} \\ \hline
\multicolumn{1}{|l|}{\cellcolor[HTML]{EFEFEF}8} & \multicolumn{1}{l|}{CNN as baseline for image classification} & \multicolumn{1}{l|}{\begin{tabular}[c]{@{}l@{}}Indication of how good the dataset is and how well \\ object detection can work\end{tabular}} \\ \hline
\multicolumn{1}{|l|}{\cellcolor[HTML]{EFEFEF}9} & \multicolumn{1}{l|}{Write interim report} & \multicolumn{1}{l|}{} \\ \hline
\multicolumn{1}{|l|}{\cellcolor[HTML]{EFEFEF}10/11} & \multicolumn{1}{l|}{\begin{tabular}[c]{@{}l@{}}fast-RCNN or YOLO for 2D object \\ detection\end{tabular}} & \multicolumn{1}{l|}{\begin{tabular}[c]{@{}l@{}}Will be used to help 3D detection in frustum method \\ and transfer learning from 2D to 3D\end{tabular}} \\ \hline
\multicolumn{1}{|l|}{\cellcolor[HTML]{EFEFEF}12/13} & \multicolumn{1}{l|}{\begin{tabular}[c]{@{}l@{}}Baseline 3D object detector using point\\ cloud data/ start making project\\ presentation\end{tabular}} & \multicolumn{1}{l|}{\begin{tabular}[c]{@{}l@{}}Indication of how well 3D object detection can work \\ with full amounts of data\end{tabular}} \\ \hline
\multicolumn{1}{|l|}{\cellcolor[HTML]{EFEFEF}14/15} & \multicolumn{1}{l|}{Encorporate 2D data using frustum method} & \multicolumn{1}{l|}{Make 3D detector better with 2D data} \\ \hline
\multicolumn{1}{|l|}{\cellcolor[HTML]{EFEFEF}15} & \multicolumn{1}{l|}{\begin{tabular}[c]{@{}l@{}}Explore whether masks of 2D object \\ further improve frustum method\end{tabular}} & \multicolumn{1}{l|}{Make 3D detector better with 2D data} \\ \hline
\multicolumn{1}{|l|}{\cellcolor[HTML]{EFEFEF}16} & \multicolumn{1}{l|}{\begin{tabular}[c]{@{}l@{}}Explore how many objects are needed \\ for the transfer learning of new 3D\\  objects\end{tabular}} & \multicolumn{1}{l|}{\begin{tabular}[c]{@{}l@{}}Use as little 3D data as possible to detect new \\ objects. This is what will happen when Fracture \\ is presented with a new project\end{tabular}} \\ \hline
\multicolumn{1}{|l|}{\cellcolor[HTML]{EFEFEF}17/18} & \multicolumn{1}{l|}{\begin{tabular}[c]{@{}l@{}}Transfer learning of 2D detection to 3D \\ space/ find common representation/ spend \\ some time writing draft section\end{tabular}} & \multicolumn{1}{l|}{\begin{tabular}[c]{@{}l@{}}Use as little 3D data as possible, and utilise \\ available 2D data\end{tabular}} \\ \hline
\multicolumn{1}{|l|}{\cellcolor[HTML]{EFEFEF}19} & \multicolumn{1}{l|}{`Cut and Paste' data generation in 3D} & \multicolumn{1}{l|}{Use as little real 3D data as possible} \\ \hline
\multicolumn{1}{|l|}{\cellcolor[HTML]{EFEFEF}20/21/22/EV1} & \multicolumn{1}{l|}{\begin{tabular}[c]{@{}l@{}}Finalise report/ poster and allow for some contingency \\ time\end{tabular}} & \multicolumn{1}{l|}{} \\ \hline
\end{tabular}%
}
\end{table}



\section{Progress}
\subsection*{Datasets}
The KITTI benchmark suit \cite{KITTI} is an autonomous driving dataset with 200,000 3D object annotations captured in cluttered scenarios, with up to 15 cars and 30 pedestrians in each image. The data is obtained from a stereo camera and LIDAR sensor mounted on top of a car that is driven in the real world. Although the KITTI benchmark suit is a rich 3D object dataset, it is not as directly applicable to mixed reality application due to the sensing quality differences between LIDAR and the portable depth camera used in mixed reality. As well as KITTI only focusing on 8 autonomous driving classes such as pedestrians, cars and bicycles. 

\cite{3D_dataset} is a large-scale 3D object dataset with 32040 object poses and 45 different objects. The point cloud data is triangulated from 11 different views, making highly detailed scenes. The scenes are controlled and do not represent what would be captured from mixed reality depth sensors due to the triangulated different views.

SUNRGBD benchmark suite \cite{SUNRGBD} is a 3D object dataset consisting of 10,335 images with 64,595 3D object bounding boxes. The data is collected on various portable RGBD cameras such as the Kinect device, with indoor scenes focusing on objects such as doors, tables and chairs. A similar quality popular dataset is the Pascal Visual Object Classes (VOC)2012 \cite{pascal-voc-2012}, which consists of 11,530 annotated object images, indoors and outdoors with 20 classes such as chairs, cars, dogs. However, PASCAL VOC 2012 only consists of 2D data. \cite{PASCAL_3D} extends the PASCAL VOC 2012 dataset with proposed 3D CAD style projections of the 2D objects. Although rich, this 3D data of the object is dissimilar to that of a depth sensor, leaving SUNRGBD the most suitable starting dataset for this project.

\begin{wrapfigure}{r}{0.4\textwidth}
  \begin{center}
    \includegraphics[width=0.38\textwidth]{images/SUN_examples.jpg}
  \end{center}
  \caption{Examples of SUNRGBD datasets RGB images with 2D annotations in red.}
  \label{fig:SUN_examples}
\end{wrapfigure}

To easily access relevant parts of the dataset a pipeline is made; RGD image, depth image, point cloud, 2D labels and bounding box and 3D labels and bounding box. This data pipeline also includes cropping images to the objects within them, for training an image classifier, and also includes taking a subset of object labels and a subset of these images. This is important, as it simplifies the problem when testing which methods are feasible, before training on the whole dataset. Figure \ref{fig:SUN_examples} shows a couple of example images with 2D bounding box annotations from the SUNRGBD dataset.

Figure \ref{fig:bad_crops} shows some examples of objects from the SUNRGBD data set where the bounding box labels are inaccurate or only partially cover the object. This could be a weakness of any system trained using bad data and there could be potential into cleaning the dataset to address this.

\begin{figure}[H]
  \begin{center}
    \includegraphics[width=0.6\textwidth]{images/bad_crops.jpg}
  \end{center}
  \caption{Cropped examples of `chairs' in the SUNRGBD dataset. Showcasing where bad labelling can occur using the 2D annotations.}
  \label{fig:bad_crops}
\end{figure}

\subsection*{Image Classification of SUNRGBD objects}
An image classifier is trained on a subset of object classes. As the easiest task, image classification gives an indication of the quality of the dataset and it's annotations, as well as giving an upper bound on what accuracy any object detectors can achieve. Images in figure \ref{fig:bad_crops} and similar are included in the training and validation.

The state of the art InceptionV3 \cite{incepV3} CNN architecture is used to demonstrate image classification on a subset of the SUNRGB dataset. 500 examples from the classes: chair, door and table are used. In the SUNRGBD dataset, images contain multiple of the same class objects, for example a room full of the same chair. To give a more general representation of objects and the whole dataset, only one object from each image is sampled in a random order of the images. To reduce training time and improve accuracy, weights pre-trained on ImageNet are used. Since ImageNet has 1000 classes, the feature extraction from the network is very general and should apply well to this similar classification task. Transfer learning of the pre-trained InceptionV3 is achieved by truncating the final classification layer and replacing it with the new desired classes. Re-training the weights is achieved by back propagation of the whole network, with batch normalisation and using the cross-entropy loss function. Figure \ref{fig:incept_trans} shows the accuracy of training and validation sets during transfer learning. The model achieves around 94\% accuracy on the validation set. This is very high, and is as expected since the task is very similar to the ImageNet classification task the model was pre-trained on.

\begin{figure}[H]
  \begin{center}
    \includegraphics[width=0.6\textwidth]{images/inception_SUNRGBD.jpg}
  \end{center}
  \caption{Accuracy for train set (orange) and validation set (blue) for transfer learning of InceptionV3 with weights pre trained on ImageNet, using 500 examples of chair, door and table each from SUNRGBD dataset to re-train. Both curves are smoothed, with the lighter background colour representing the actual data. Training is very quick. Training and validation accuracies start at around 30\% (cropped from graph for scale issues). The model starts to over-fit after around 800 epochs, where the validation accuracy is seen to decrease from it's peak just above 94\% accuracy.}
  \label{fig:incept_trans}
\end{figure}

%Create the style, and include the bibliography.
\bibliography{interim}
\bibliographystyle{abbrv}

% the end
\end{document}