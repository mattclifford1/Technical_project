\documentclass[11pt]{article}

% First load extension packages
\usepackage[a4paper,margin=25mm]{geometry}    % page layout
\usepackage{setspace} \onehalfspacing         % line spacing
\usepackage{amsfonts,amssymb,amsmath}         % useful math extensions
\usepackage{graphicx}                         % graphics import
\usepackage{siunitx}                          % easy SI units

% Change paragraph indentation
\setlength{\parskip}{10pt}
\setlength{\parindent}{0pt}

% User-defined commands
\newcommand{\diff}[2]{\frac{\mathrm{d}{#1}}{\mathrm{d}{#2}}}
\newcommand{\ddiff}[2]{\frac{\mathrm{d}^2{#1}}{\mathrm{d}{#2}^2}}
\newcommand{\pdiff}[2]{\frac{\partial{#1}}{\partial{#2}}}
\newcommand{\pddiff}[2]{\frac{\partial^2{#1}}{\partial{#2}^2}}
\newcommand{\pdiffdiff}[3]{\frac{\partial^2{#1}}{\partial{#2}\partial{#3}}}
\renewcommand{\vec}[1]{\boldsymbol{#1}}
\newcommand{\Idx}{\;\mathrm{d}x}
\newcommand{\Real}{\mathbb{R}}
\newcommand{\Complex}{\mathbb{C}}
\newcommand{\Rational}{\mathbb{Q}}
\newcommand{\Integer}{\mathbb{Z}}
\newcommand{\Natural}{\mathbb{N}}

% topmatter
\title{3D Object Detection in the Wild}

\author{Matt Clifford \\ Supervised by Dr R.\ Santos-Rodriguez}

\date{\today}

% main body
\begin{document}
\maketitle

\section{Introduction}
With recent advancements in mixed reality technology $[googleglasses and hololens]$, there is an interest in the use of mixed reality to help create `smart spaces', where mixed reality users are informed and guided through environments unknown to them.

Fracture Reality $[fracturereality.io]$, has specific interest in 3D object detection for future projects. They specialise in creating bespoke mixed reality software for both private and government sectors. They mostly work with mixed reality visualisations of maps of an environment, for example to aid the control centres in airports $[https://www.youtube.com/watch?time_continue=2&v=t5L-jBLD04I]$. Although they have many projects that would benefit from 3D object detection, an ongoing project investigating how the affect of mixed reality is in tackling circulation issues $[REF]$. The use of 3D object detection would identify and locate objects such as stairs, doors, elevators and escalators. This aids the user in identifying if these objects are correct pathway for them, addressing bottle neck problems. A variant use of this would be to help the visually or navigationally impaired, with mixed reality help from detected objects of importance.

The objects of interest for the object detection are on a case to case basis, due to specific needs of each project. Fracture Reality are able to create some object specific use case data, but in the region of hundreds of examples, and due to the specificity of each task, finding existing object datasets might not be possible. This leaves retraining a detector for every new object is infeasible. The need for a system that can detect objects given as little training examples as possible is therefore needed. This can be achieved by utilising general knowledge learnt from similar or relevant tasks and quickly applying it to the specific task of interest \cite{DeCAF}.


\section{Literature review}
In 2012, Alex Krizhevsky et al. revolutionised computer vision with a convolution neural network (CNN), inspired by \cite{Yann}, which performed image classification on the ImageNet dataset [REF]. The CNN, named AlexNet, consists of 5 convolution layers followed by 3 fully-connected layers. It won ImageNet's ILSVRC-2010 and ILSVRC-2012 image classification contests \cite{alex_net}. In \cite{alex_net}, they claim `All of our experiments suggest that our results can be improved simply by waiting for faster GPUs and bigger datasets to become available', and since AlexNet's success, there have been consistent advancements in CNN state of the art. In 2014, K. Simonyan and A. Zisserman introduced VGG16\cite{VGG16}, consisting of 16 convolution layers followed by 3 fully-connected layers. VGG16 achieves a top-5 error rate of 7.4\% on ILSVRC-2014 compared to AlexNet's 17.0\%. \cite{VGG16} achieves this performance boost by using smaller convolution filters, 3x3 compared to AlexNet's 11x11 which decreases the number of weights to train at each convolution layer, alongside a deeper convolution architecture, which can extract deeper relevant semantic meaning from the images. Further performance improvements were made by the use of `inception models' by C. Szegedy et al \cite{inception]$\cite{inceptionV2}, which stack the outputs of several convolutions of the same input, followed by filter concatenation. As well as `residual learning'\cite{ResNet}, which connects the outputs of multiple convolution layer. A combination of `inception' and `residual' models was formulated by C. Szegedy et al \cite{Incep_ResNet}. Further improvements to 2D image classification include \cite{neural_search} \cite{scaleable_image}.

To identify the location of objects in images, \cite{RNN} uses a regional CNN (RCNN) model. Regional proposals of the image, which are run though a modified AlexNet. The positive classification results from the regional proposals are then adjusted using a linear regression model to obtain better object bounding boxes. Computational speed ups are proposed in Fast-RNN \cite{fast_RNN}, which pools the regional proposals. Further computational speed up are proposed in Faster-RNN \cite{faster_RNN}, which combines the selective search regional proposals into the CNN. \cite{YOLO} proposes a computational speed up by using a search grid rather than regional proposals, an approach known as `you only look once' (YOLO).

\cite{complex_YOLO} proposes a method of using the YOLO object detection approach in 3D space named Complex-YOLO, using only point cloud data from LIDAR depth sensors. Complex-YOLO uses a Euler-Region Proposal Network which estimates the orientation of objects by adding an imaginary and real part for each proposal box box. This results in 5 times speed up in object detection from the previous state of the art, with on par or better accuracy evaluated on only LIDAR data from KITTI benchmark suit \cite{KITTI}. The KITTI benchmark suit is an autonomous driving dataset with 200,000 3D object annotations captured in cluttered scenarios, with up to 15 cars and 30 pedestrians in each image. The data is obtained from a stereo camera and LIDAR sensor mounted on top of a car that is driven in the real world. Although the KITTI benchmark suit is a rich 3D object dataset, it is not as directly applicable to mixed reality application due to the sensing quality differences between LIDAR and portable depth camera. As well as KITTI only focusing on 8 autonomous driving classes such as pedestrians, cars and bicycles. \cite{3D_dataset} is a large-scale 3D object dataset with 32040 object poses and 45 different
object. The point cloud data is triangulated from 11 different views, making highly detailed scenes. The scenes are controlled and do not represent what would be captured in the wild. SUNRGBD benchmark suite \cite{SUNRGBD} is a 3D object dataset consisting of 10,335 images with 64,595 3D object bounding boxes. The data is collected on various portable RGBD cameras such as the Kinect device, with indoor scenes focusing on objects such as doors, tables and chairs. A similar quality popular dataset is the Pascal Visual Object Classes (VOC)2012 \cite{pascal-voc-2012}, which consists of 11,530 annotated object images, indoors and outdoors with 20 classes such as chairs, cars, dogs. However, PASCAL VOC 2012 only consists of 2D data. \cite{PASCAL_3D} extends the PASCAL VOC 2012 dataset with proposed 3D CAD style projections of the 2D objects. Although rich, this 3D data of the object is dissimilar to that of a depth sensor, leaving SUNRGBD the most suitable starting dataset for this project.

\cite{cut_paste} proposes a `cut and paste' style approach to synthesising 2D object detection datasets. First a object mask is predicted for the object, which is then applied to the image to `cut and paste' the object into background scenes. Occlusions, truncations and blending are then applied to the object to help it more naturally fit into the scene. \cite{synthetic_train} shows that training scene detectors on synthetic data produces comparable results on real life tests to detectors trained on the SUNRGBD state of the art dataset.

Current state of the art 3D object mostly detectors use LIDAR derived point clouds, trained on the KITTI benchmark suite \cite{point_fusion}\cite{multi_fusion}\cite{fast_furious}\cite{VoxelNet} or controlled, detailed objects \cite{subgroup_voting}\cite{pose_RGBD}\cite{real_time_single}. \cite{PointNets} and \cite{frustum} use 2D object detectors to aid the regional proposal of the 3D object, by searching only the 3D space in the point cloud occupied from the projected frustum obtained from the 2D object detector. This vastly reduces the search space and determines the object class for the 3D detector, resulting in improved speed and detection compared to using point cloud data alone, especially if the object suffers from occlusions or has a sparse representation. An alternative approach could combine object mask detectors\cite{first_person_mask}\cite{MASK-RCNN} with the 3D projection of the mask to help further refine the 3D object search space. \cite{latent_surface} uses latent support surfaces for 3D object detection on the SUNRGBD dataset. Another notable 3D object detector that use the SUNRGBD dataset is \cite{SnapNet}. 

Transfer learning is a well studied area of deep learning \cite{DeCAF}\cite{survery_on_transfer}\cite{how_transferable}, where a network trained for a specific task is re-purposed for a similar task. This is often achieved by truncating the last few layers of a pre-trained network where the network is specific to the trained task, and keeping the starting layers that have more general representations. Since the start of the network is already trained for general tasks relevant to both the original training task and the new desired task, only the last few layers need to be re-trained for the new task. This can be done using considerably less training data than the original network was trained with.

Common representation -- do a little research,,
	Auto encoders to possibly solve this 

Unsupervised learning for creating data cheaply


\section{Project plan}

- collect dataset

- CNN as baseline for classification

- faster-RCNN as baseline for 2D object detection

- Make baseline 3D object detector using point cloud data

- Transfer 2D model to 3D, elaborate on this

- Assess results of transfer learning 3D detector with a few 3D new object examples

- Auto-encoder to find common representation between 2D and 3D?

- Generating objects in scenes using cut and paste in 3D?

\section{Progress}

Extracted relevant annotations and labels form SUNRGBD dataset
Using 2D annotations, only take a subset of objects and crop full image to just the object area.
Train CNN on these cropped images to have a baseline 2D classifier.

- show some examples of the dataset with annotations

- show examples of cropped inputs

- discuss how this will help with RCNN

- how RCNN will be used for the frustum method

\newpage
%Create the style, and include the bibliography.
\bibliography{biblio.bib}
\bibliographystyle{plain}

% the end
\end{document}